{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model will still be a variational autoencoder, but I will use a CNN within the architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_channels=2, input_length=200, h_dim=128, z_dim=20):\n",
    "        super().__init__()\n",
    "\n",
    "        # encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, 16, kernel_size=3, stride=2, padding=1), # (N, 16, 100)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # code\n",
    "        self.fc_mu = nn.Linear(64 * 25, z_dim)\n",
    "        self.fc_sigma = nn.Linear(64 * 25, z_dim)\n",
    "        self.fc_z = nn.Linear(z_dim, 64 * 25)\n",
    "\n",
    "        # decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(16, input_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h_flat = self.flatten(h)\n",
    "        mu = self.fc_mu(h_flat)\n",
    "        sigma = self.fc_sigma(h_flat)\n",
    "        return mu, sigma\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.fc_z(z)\n",
    "        h = h.view(h.size(0), 64, 25)\n",
    "        x_reconstructed = self.decoder(h)\n",
    "        return x_reconstructed\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, sigma = self.encode(x)\n",
    "        epsilon = torch.randn_like(sigma)\n",
    "        z_new = mu + sigma * epsilon\n",
    "        x_reconstructed = self.decode(z_new)\n",
    "        return x_reconstructed, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 200])\n",
      "torch.Size([4, 20])\n",
      "torch.Size([4, 20])\n"
     ]
    }
   ],
   "source": [
    "# testing on a randon data\n",
    "x = torch.randn(4, 2, 200)\n",
    "vae = VariationalAutoEncoder()\n",
    "x_reconstructed, mu, sigma = vae(x)\n",
    "print(x_reconstructed.shape)\n",
    "print(mu.shape)\n",
    "print(sigma.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "z_dim=40\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "lr_rate = 3e-5\n",
    "weight_decay = None\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset loading\n",
    "background = np.load('../data/background.npz')['data']\n",
    "bbh = np.load('../data/bbh_for_challenge.npy')\n",
    "sglf = np.load('../data/sglf_for_challenge.npy')\n",
    "\n",
    "def normalize(data):\n",
    "    stds = np.std(data, axis=-1, keepdims=True)\n",
    "    return data / stds\n",
    "\n",
    "background = normalize(background)\n",
    "bbh = normalize(bbh)\n",
    "sglf = normalize(sglf)\n",
    "\n",
    "x_train, x_test = train_test_split(background, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
    "x_test = torch.tensor(x_test, dtype=torch.float32)\n",
    "bbh = torch.tensor(bbh, dtype=torch.float32)\n",
    "sglf = torch.tensor(sglf, dtype=torch.float32)\n",
    "\n",
    "train_loader = DataLoader(dataset=x_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model setup\n",
    "model = VariationalAutoEncoder(z_dim=z_dim).to(device)\n",
    "if weight_decay is None:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr_rate)\n",
    "else:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr_rate, weight_decay=weight_decay)\n",
    "loss_fn = nn.MSELoss(reduction=\"sum\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2500it [00:08, 289.29it/s, loss=1.32e+4]\n",
      "2500it [00:09, 274.31it/s, loss=1.28e+4]\n",
      "2500it [00:08, 286.99it/s, loss=1.28e+4]\n",
      "2500it [00:08, 282.05it/s, loss=1.28e+4]\n",
      "2500it [00:09, 277.32it/s, loss=1.28e+4]\n",
      "2500it [00:08, 286.99it/s, loss=1.28e+4]\n",
      "2500it [00:09, 270.56it/s, loss=1.28e+4]\n",
      "2500it [00:08, 292.03it/s, loss=1.28e+4]\n",
      "2500it [00:08, 278.99it/s, loss=1.28e+4]\n",
      "2500it [00:09, 270.99it/s, loss=1.28e+4]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    loop = tqdm(enumerate(train_loader))\n",
    "    for i, x in loop:\n",
    "        # forward pass\n",
    "        x = x.to(device) \n",
    "        x_reconstructed, mu, sigma = model(x)\n",
    "\n",
    "        # compute loss\n",
    "        reconstruction_loss = loss_fn(x_reconstructed, x)\n",
    "        kl_div = -torch.sum(1 + torch.log(sigma.pow(2)) - mu.pow(2) - sigma.pow(2))\n",
    "\n",
    "        # backprop\n",
    "        loss = reconstruction_loss + kl_div # could add an alpha term to make it a disentangled autoencoder\n",
    "        optimizer.zero_grad() # ensure no gradients before\n",
    "        loss.backward() \n",
    "        optimizer.step() # compute new gradients\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reconstruction_errors(model, data):\n",
    "    model.eval()\n",
    "    data = data.to(device) \n",
    "    num_samples = data.size(0)\n",
    "    with torch.no_grad():\n",
    "        reconstructed, mu, sigma = model(data)\n",
    "        reconstruction_loss = loss_fn(reconstructed, data) / num_samples\n",
    "        kl_div = -torch.sum(1 + torch.log(sigma.pow(2)) - mu.pow(2) - sigma.pow(2)) / num_samples\n",
    "        total_loss = reconstruction_loss + kl_div\n",
    "    return reconstruction_loss.item(), kl_div.item(), total_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test.to(device)\n",
    "sglf = sglf.to(device)\n",
    "bbh = bbh.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set - Reconstruction Loss: 400.011962890625, KL Divergence: 0.1897244155406952, Total Loss: 400.2016906738281\n",
      "SGLF Set - Reconstruction Loss: 399.98150634765625, KL Divergence: 0.17907898128032684, Total Loss: 400.16058349609375\n",
      "BBH Set - Reconstruction Loss: 400.5328674316406, KL Divergence: 0.14685750007629395, Total Loss: 400.6797180175781\n"
     ]
    }
   ],
   "source": [
    "test_rec_loss, test_kl_loss, test_total_loss = compute_reconstruction_errors(model, x_test)\n",
    "sglf_rec_loss, sglf_kl_loss, sglf_total_loss = compute_reconstruction_errors(model, sglf)\n",
    "bbh_rec_loss, bbh_kl_loss, bbh_total_loss = compute_reconstruction_errors(model, bbh)\n",
    "\n",
    "print(f\"Test Set - Reconstruction Loss: {test_rec_loss}, KL Divergence: {test_kl_loss}, Total Loss: {test_total_loss}\")\n",
    "print(f\"SGLF Set - Reconstruction Loss: {sglf_rec_loss}, KL Divergence: {sglf_kl_loss}, Total Loss: {sglf_total_loss}\")\n",
    "print(f\"BBH Set - Reconstruction Loss: {bbh_rec_loss}, KL Divergence: {bbh_kl_loss}, Total Loss: {bbh_total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"VAE_increased_zdim_40_wd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_model_results(model_name, z_dim, batch_size, epochs, lr_rate, weight_decay, test_rec_loss, test_kl_loss, test_total_loss, sglf_rec_loss, sglf_kl_loss, sglf_total_loss, bbh_rec_loss, bbh_kl_loss, bbh_total_loss, file_name=\"vae_model_results.csv\"):\n",
    "    log_entry = {\n",
    "        \"model_name\": model_name,\n",
    "        \"z_dim\": z_dim,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": epochs,\n",
    "        \"lr_rate\": lr_rate,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"test_rec_loss\": test_rec_loss,\n",
    "        \"test_kl_loss\": test_kl_loss,\n",
    "        \"test_total_loss\": test_total_loss,\n",
    "        \"sglf_rec_loss\": sglf_rec_loss,\n",
    "        \"sglf_kl_loss\": sglf_kl_loss, \n",
    "        \"sglf_total_loss\": sglf_total_loss, \n",
    "        \"bbh_rec_loss\": bbh_rec_loss, \n",
    "        \"bbh_kl_loss\": bbh_kl_loss, \n",
    "        \"bbh_total_loss\": bbh_total_loss,\n",
    "    }\n",
    "    \n",
    "    file_exists = False\n",
    "    try:\n",
    "        with open(file_name, mode='r', newline='') as file:\n",
    "            file_exists = True \n",
    "    except FileNotFoundError:\n",
    "        file_exists = False \n",
    "\n",
    "    with open(file_name, mode='a', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=log_entry.keys())\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(log_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_model_results(model_name, z_dim, batch_size, num_epochs, lr_rate, weight_decay, test_rec_loss, test_kl_loss, test_total_loss, sglf_rec_loss, sglf_kl_loss, sglf_total_loss, bbh_rec_loss, bbh_kl_loss, bbh_total_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smash",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
