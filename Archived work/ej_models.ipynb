{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "background = np.load(\"background.npz\")\n",
    "background_wash = pd.DataFrame()\n",
    "background_lou = pd.DataFrame()\n",
    "def normalize(data):\n",
    "    stds = np.std(data, axis=-1, keepdims=True)\n",
    "    return data / stds\n",
    "for key in background:\n",
    "    background_wash = background[key][:, 0, :]\n",
    "    background_wash = torch.tensor(normalize(background_wash), dtype=torch.float32)\n",
    "    background_lou = background[key][:, 1, :]\n",
    "    background_lou = torch.tensor(normalize(background_lou), dtype=torch.float32)\n",
    "\n",
    "bbh_for_challenge = np.load(\"bbh_for_challenge.npy\")\n",
    "bbh_wash = bbh_for_challenge[:, 0, :]\n",
    "bbh_wash = torch.tensor(normalize(bbh_wash), dtype=torch.float32)\n",
    "bbh_lou = bbh_for_challenge[:, 1, :]\n",
    "bbh_lou = torch.tensor(normalize(bbh_lou), dtype=torch.float32)\n",
    "\n",
    "sglf_for_challenge = np.load(\"sglf_for_challenge.npy\")\n",
    "sglf_wash = sglf_for_challenge[:, 0, :]\n",
    "sglf_wash = torch.tensor(normalize(sglf_wash), dtype=torch.float32)\n",
    "sglf_lou = sglf_for_challenge[:, 1, :]\n",
    "sglf_lou = torch.tensor(normalize(sglf_lou), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/2106.02770 \n",
    "\n",
    "For my current lab project i'm utilizing this neural process model used in spatiotemporal ML/climate visualizations, which also has a (latent) encoder and a decoder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 4), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(4, input_dim),  \n",
    "            nn.Sigmoid() \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy-Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnergyModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(EnergyModel, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Autoencoder for Wash:\n",
      "Epoch 1/20, Loss: 1.1931\n",
      "Epoch 2/20, Loss: 1.0721\n",
      "Epoch 3/20, Loss: 1.0178\n",
      "Epoch 4/20, Loss: 1.0020\n",
      "Epoch 5/20, Loss: 0.9965\n",
      "Epoch 6/20, Loss: 0.9941\n",
      "Epoch 7/20, Loss: 0.9938\n",
      "Epoch 8/20, Loss: 0.9922\n",
      "Epoch 9/20, Loss: 0.9921\n",
      "Epoch 10/20, Loss: 0.9915\n",
      "Epoch 11/20, Loss: 0.9909\n",
      "Epoch 12/20, Loss: 0.9904\n",
      "Epoch 13/20, Loss: 0.9899\n",
      "Epoch 14/20, Loss: 0.9901\n",
      "Epoch 15/20, Loss: 0.9900\n",
      "Epoch 16/20, Loss: 0.9896\n",
      "Epoch 17/20, Loss: 0.9884\n",
      "Epoch 18/20, Loss: 0.9883\n",
      "Epoch 19/20, Loss: 0.9895\n",
      "Epoch 20/20, Loss: 0.9881\n",
      "\n",
      "Training Autoencoder for Lou:\n",
      "Epoch 1/20, Loss: 1.2053\n",
      "Epoch 2/20, Loss: 1.0812\n",
      "Epoch 3/20, Loss: 1.0193\n",
      "Epoch 4/20, Loss: 1.0030\n",
      "Epoch 5/20, Loss: 0.9966\n",
      "Epoch 6/20, Loss: 0.9949\n",
      "Epoch 7/20, Loss: 0.9934\n",
      "Epoch 8/20, Loss: 0.9921\n",
      "Epoch 9/20, Loss: 0.9919\n",
      "Epoch 10/20, Loss: 0.9905\n",
      "Epoch 11/20, Loss: 0.9906\n",
      "Epoch 12/20, Loss: 0.9905\n",
      "Epoch 13/20, Loss: 0.9898\n",
      "Epoch 14/20, Loss: 0.9893\n",
      "Epoch 15/20, Loss: 0.9894\n",
      "Epoch 16/20, Loss: 0.9893\n",
      "Epoch 17/20, Loss: 0.9886\n",
      "Epoch 18/20, Loss: 0.9892\n",
      "Epoch 19/20, Loss: 0.9897\n",
      "Epoch 20/20, Loss: 0.9887\n",
      "\n",
      "Training Energy-Based Model for Wash:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hapah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([200])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 1.0020\n",
      "Epoch 2/20, Loss: 1.0005\n",
      "Epoch 3/20, Loss: 1.0004\n",
      "Epoch 4/20, Loss: 1.0003\n",
      "Epoch 5/20, Loss: 1.0002\n",
      "Epoch 6/20, Loss: 1.0001\n",
      "Epoch 7/20, Loss: 1.0001\n",
      "Epoch 8/20, Loss: 1.0001\n",
      "Epoch 9/20, Loss: 1.0001\n",
      "Epoch 10/20, Loss: 1.0001\n",
      "Epoch 11/20, Loss: 1.0001\n",
      "Epoch 12/20, Loss: 1.0001\n",
      "Epoch 13/20, Loss: 1.0001\n",
      "Epoch 14/20, Loss: 1.0001\n",
      "Epoch 15/20, Loss: 1.0001\n",
      "Epoch 16/20, Loss: 1.0000\n",
      "Epoch 17/20, Loss: 1.0000\n",
      "Epoch 18/20, Loss: 1.0000\n",
      "Epoch 19/20, Loss: 1.0000\n",
      "Epoch 20/20, Loss: 1.0000\n",
      "\n",
      "Training Energy-Based Model for Lou:\n",
      "Epoch 1/20, Loss: 1.0063\n",
      "Epoch 2/20, Loss: 1.0006\n",
      "Epoch 3/20, Loss: 1.0004\n",
      "Epoch 4/20, Loss: 1.0003\n",
      "Epoch 5/20, Loss: 1.0002\n",
      "Epoch 6/20, Loss: 1.0002\n",
      "Epoch 7/20, Loss: 1.0002\n",
      "Epoch 8/20, Loss: 1.0001\n",
      "Epoch 9/20, Loss: 1.0001\n",
      "Epoch 10/20, Loss: 1.0001\n",
      "Epoch 11/20, Loss: 1.0001\n",
      "Epoch 12/20, Loss: 1.0001\n",
      "Epoch 13/20, Loss: 1.0001\n",
      "Epoch 14/20, Loss: 1.0001\n",
      "Epoch 15/20, Loss: 1.0001\n",
      "Epoch 16/20, Loss: 1.0000\n",
      "Epoch 17/20, Loss: 1.0000\n",
      "Epoch 18/20, Loss: 1.0000\n",
      "Epoch 19/20, Loss: 1.0000\n",
      "Epoch 20/20, Loss: 1.0000\n",
      "\n",
      "Evaluating Background Wash with Autoencoder:\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 116\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, data, autoencoder, energy_model \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with Autoencoder:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 116\u001b[0m     recon_errors, threshold_ae \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mautoencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoencoder - Threshold: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthreshold_ae\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with Energy-Based Model:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[78], line 84\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, test_loader, percentile_threshold)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, Autoencoder):\n\u001b[0;32m     83\u001b[0m     reconstructed \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m---> 84\u001b[0m     error \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreconstructed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mextend(error\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "train_wash, test_wash = random_split(background_wash, [80000, 20000])\n",
    "train_lou, test_lou = random_split(background_wash, [80000, 20000])\n",
    "batch_size = 128\n",
    "train_loader_wash = DataLoader(train_wash, batch_size=batch_size, shuffle=True)\n",
    "test_loader_wash = DataLoader(test_wash, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_loader_lou = DataLoader(train_lou, batch_size=batch_size, shuffle=True)\n",
    "test_loader_lou = DataLoader(test_lou, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=20):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            inputs = batch[0]\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "input_dim = background_wash.shape[1]\n",
    "autoencoder_wash = Autoencoder(input_dim)\n",
    "criterion_ae = nn.MSELoss()\n",
    "optimizer_ae_wash = optim.Adam(autoencoder_wash.parameters(), lr=0.001)\n",
    "\n",
    "print(\"\\nTraining Autoencoder for Wash:\")\n",
    "train_model(autoencoder_wash, train_loader_wash, criterion_ae, optimizer_ae_wash, epochs=20)\n",
    "\n",
    "autoencoder_lou = Autoencoder(input_dim)\n",
    "optimizer_ae_lou = optim.Adam(autoencoder_lou.parameters(), lr=0.001)\n",
    "\n",
    "print(\"\\nTraining Autoencoder for Lou:\")\n",
    "train_model(autoencoder_lou, train_loader_lou, criterion_ae, optimizer_ae_lou, epochs=20)\n",
    "\n",
    "energy_model_wash = EnergyModel(input_dim)\n",
    "criterion_energy = nn.MSELoss()\n",
    "optimizer_energy_wash = optim.Adam(energy_model_wash.parameters(), lr=0.001)\n",
    "\n",
    "print(\"\\nTraining Energy-Based Model for Wash:\")\n",
    "train_model(energy_model_wash, train_loader_wash, criterion_energy, optimizer_energy_wash, epochs=20)\n",
    "\n",
    "# 4. Energy-Based Model for lou\n",
    "energy_model_lou = EnergyModel(input_dim)\n",
    "optimizer_energy_lou = optim.Adam(energy_model_lou.parameters(), lr=0.001)\n",
    "\n",
    "print(\"\\nTraining Energy-Based Model for Lou:\")\n",
    "train_model(energy_model_lou, train_loader_lou, criterion_energy, optimizer_energy_lou, epochs=20)\n",
    "\n",
    "# def evaluate_model(model, test_loader):\n",
    "#     model.eval()\n",
    "#     results = []\n",
    "#     with torch.no_grad():\n",
    "#         # If test_loader is a tensor, convert it to a DataLoader\n",
    "#         if isinstance(test_loader, torch.Tensor):\n",
    "#             test_loader = DataLoader(test_loader, batch_size=128, shuffle=False)\n",
    "        \n",
    "#         for batch in test_loader:\n",
    "#             # Ensure inputs have the right shape\n",
    "#             inputs = batch if isinstance(batch, torch.Tensor) else batch[0]\n",
    "#             if isinstance(model, Autoencoder):\n",
    "#                 reconstructed = model(inputs)\n",
    "#                 error = torch.mean((reconstructed - inputs) ** 2, dim=1)\n",
    "#                 results.extend(error.numpy())\n",
    "#             else:\n",
    "#                 energy = model(inputs).squeeze()\n",
    "#                 results.extend(energy.numpy())\n",
    "#     return results\n",
    "\n",
    "def evaluate_model(model, test_loader, percentile_threshold=99):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        # If test_loader is a tensor, convert it to a DataLoader\n",
    "        if isinstance(test_loader, torch.Tensor):\n",
    "            test_loader = DataLoader(test_loader, batch_size=128, shuffle=False)\n",
    "        \n",
    "        for batch in test_loader:\n",
    "            inputs = batch if isinstance(batch, torch.Tensor) else batch[0]\n",
    "            if isinstance(model, Autoencoder):\n",
    "                reconstructed = model(inputs)\n",
    "                error = torch.mean((reconstructed - inputs) ** 2, dim=1)\n",
    "                predictions.extend(error.numpy())\n",
    "            else:\n",
    "                energy = model(inputs).squeeze()\n",
    "                predictions.extend(energy.numpy())\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    threshold = np.percentile(predictions, percentile_threshold)\n",
    "    anomalies = predictions > threshold\n",
    "    print(f\"Detected {np.sum(anomalies)} anomalies out of {len(predictions)} samples.\")\n",
    "    return predictions, threshold\n",
    "\n",
    "datasets = [\n",
    "    (\"Background Wash\", test_wash, autoencoder_wash, energy_model_wash),\n",
    "    (\"Background Lou\", test_lou, autoencoder_lou, energy_model_lou),\n",
    "    (\"BBH Wash\", bbh_wash, autoencoder_wash, energy_model_wash),\n",
    "    (\"BBH Lou\", bbh_lou, autoencoder_lou, energy_model_lou),\n",
    "    (\"SGLF Wash\", sglf_wash, autoencoder_wash, energy_model_wash),\n",
    "    (\"SGLF Lou\", sglf_lou, autoencoder_lou, energy_model_lou),\n",
    "]\n",
    "\n",
    "# for name, data, autoencoder, energy_model in datasets:\n",
    "#     print(f\"\\nEvaluating {name} with Autoencoder:\")\n",
    "#     reconstruction_errors = evaluate_model(autoencoder, data)\n",
    "#     print(f\"Reconstruction Error - Mean: {np.mean(reconstruction_errors):.4f}, Std: {np.std(reconstruction_errors):.4f}\")\n",
    "\n",
    "#     print(f\"Evaluating {name} with Energy-Based Model:\")\n",
    "#     energy_scores = evaluate_model(energy_model, data)\n",
    "#     print(f\"Energy Scores - Mean: {np.mean(energy_scores):.4f}, Std: {np.std(energy_scores):.4f}\")\n",
    "\n",
    "for name, data, autoencoder, energy_model in datasets:\n",
    "    print(f\"\\nEvaluating {name} with Autoencoder:\")\n",
    "    recon_errors, threshold_ae = evaluate_model(autoencoder, data)\n",
    "    print(f\"Autoencoder - Threshold: {threshold_ae:.4f}\")\n",
    "\n",
    "    print(f\"Evaluating {name} with Energy-Based Model:\")\n",
    "    energy_scores, threshold_energy = evaluate_model(energy_model, data)\n",
    "    print(f\"Energy Model - Threshold: {threshold_energy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scuffed Accuracy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Autoencoder for Wash:\n",
      "Epoch 1/20, Loss: 1.1273\n",
      "Epoch 2/20, Loss: 1.0097\n",
      "Epoch 3/20, Loss: 0.9958\n",
      "Epoch 4/20, Loss: 0.9927\n",
      "Epoch 5/20, Loss: 0.9914\n",
      "Epoch 6/20, Loss: 0.9904\n",
      "Epoch 7/20, Loss: 0.9898\n",
      "Epoch 8/20, Loss: 0.9898\n",
      "Epoch 9/20, Loss: 0.9891\n",
      "Epoch 10/20, Loss: 0.9888\n",
      "Epoch 11/20, Loss: 0.9884\n",
      "Epoch 12/20, Loss: 0.9884\n",
      "Epoch 13/20, Loss: 0.9887\n",
      "Epoch 14/20, Loss: 0.9875\n",
      "Epoch 15/20, Loss: 0.9875\n",
      "Epoch 16/20, Loss: 0.9877\n",
      "Epoch 17/20, Loss: 0.9869\n",
      "Epoch 18/20, Loss: 0.9877\n",
      "Epoch 19/20, Loss: 0.9871\n",
      "Epoch 20/20, Loss: 0.9867\n",
      "\n",
      "Training Autoencoder for Lou:\n",
      "Epoch 1/20, Loss: 1.1264\n",
      "Epoch 2/20, Loss: 1.0092\n",
      "Epoch 3/20, Loss: 0.9953\n",
      "Epoch 4/20, Loss: 0.9928\n",
      "Epoch 5/20, Loss: 0.9911\n",
      "Epoch 6/20, Loss: 0.9906\n",
      "Epoch 7/20, Loss: 0.9897\n",
      "Epoch 8/20, Loss: 0.9899\n",
      "Epoch 9/20, Loss: 0.9898\n",
      "Epoch 10/20, Loss: 0.9887\n",
      "Epoch 11/20, Loss: 0.9887\n",
      "Epoch 12/20, Loss: 0.9883\n",
      "Epoch 13/20, Loss: 0.9881\n",
      "Epoch 14/20, Loss: 0.9880\n",
      "Epoch 15/20, Loss: 0.9881\n",
      "Epoch 16/20, Loss: 0.9872\n",
      "Epoch 17/20, Loss: 0.9876\n",
      "Epoch 18/20, Loss: 0.9873\n",
      "Epoch 19/20, Loss: 0.9872\n",
      "Epoch 20/20, Loss: 0.9873\n",
      "Detected 5000 anomalies out of 100000 samples.\n",
      "Detected 5000 anomalies out of 100000 samples.\n",
      "tensor([0.9967, 0.9929, 0.9863,  ..., 0.9930, 0.9549, 0.9945])\n",
      "tensor([0., 0., 0.,  ..., 1., 1., 1.])\n",
      "Epoch 1 (Wash), Loss: 0.6956\n",
      "tensor([0.9967, 0.9929, 0.9863,  ..., 0.9930, 0.9549, 0.9945])\n",
      "tensor([0., 0., 0.,  ..., 1., 1., 1.])\n",
      "Epoch 2 (Wash), Loss: 0.6954\n",
      "tensor([0.9967, 0.9929, 0.9863,  ..., 0.9930, 0.9549, 0.9945])\n",
      "tensor([0., 0., 0.,  ..., 1., 1., 1.])\n",
      "Epoch 3 (Wash), Loss: 0.6952\n",
      "tensor([0.9967, 0.9929, 0.9863,  ..., 0.9930, 0.9549, 0.9945])\n",
      "tensor([0., 0., 0.,  ..., 1., 1., 1.])\n",
      "Epoch 4 (Wash), Loss: 0.6950\n",
      "tensor([0.9967, 0.9929, 0.9863,  ..., 0.9930, 0.9549, 0.9945])\n",
      "tensor([0., 0., 0.,  ..., 1., 1., 1.])\n",
      "Epoch 5 (Wash), Loss: 0.6949\n",
      "tensor([0.9967, 0.9929, 0.9863,  ..., 0.9930, 0.9549, 0.9945])\n",
      "tensor([0., 0., 0.,  ..., 1., 1., 1.])\n",
      "Epoch 6 (Wash), Loss: 0.6947\n",
      "tensor([0.9967, 0.9929, 0.9863,  ..., 0.9930, 0.9549, 0.9945])\n",
      "tensor([0., 0., 0.,  ..., 1., 1., 1.])\n",
      "Epoch 7 (Wash), Loss: 0.6946\n",
      "tensor([0.9967, 0.9929, 0.9863,  ..., 0.9930, 0.9549, 0.9945])\n",
      "tensor([0., 0., 0.,  ..., 1., 1., 1.])\n",
      "Epoch 8 (Wash), Loss: 0.6944\n",
      "tensor([0.9967, 0.9929, 0.9863,  ..., 0.9930, 0.9549, 0.9945])\n",
      "tensor([0., 0., 0.,  ..., 1., 1., 1.])\n",
      "Epoch 9 (Wash), Loss: 0.6943\n",
      "tensor([0.9967, 0.9929, 0.9863,  ..., 0.9930, 0.9549, 0.9945])\n",
      "tensor([0., 0., 0.,  ..., 1., 1., 1.])\n",
      "Epoch 10 (Wash), Loss: 0.6942\n",
      "tensor([0.9967, 0.9929, 0.9863,  ..., 0.9930, 0.9549, 0.9945])\n",
      "tensor([0., 0., 0.,  ..., 1., 1., 1.])\n",
      "Epoch 11 (Wash), Loss: 0.6941\n",
      "tensor([0.9967, 0.9929, 0.9863,  ..., 0.9930, 0.9549, 0.9945])\n",
      "tensor([0., 0., 0.,  ..., 1., 1., 1.])\n",
      "Epoch 12 (Wash), Loss: 0.6940\n",
      "tensor([0.9967, 0.9929, 0.9863,  ..., 0.9930, 0.9549, 0.9945])\n",
      "tensor([0., 0., 0.,  ..., 1., 1., 1.])\n",
      "Epoch 13 (Wash), Loss: 0.6939\n",
      "tensor([0.9967, 0.9929, 0.9863,  ..., 0.9930, 0.9549, 0.9945])\n",
      "tensor([0., 0., 0.,  ..., 1., 1., 1.])\n",
      "Epoch 14 (Wash), Loss: 0.6938\n",
      "tensor([0.9967, 0.9929, 0.9863,  ..., 0.9930, 0.9549, 0.9945])\n",
      "tensor([0., 0., 0.,  ..., 1., 1., 1.])\n",
      "Epoch 15 (Wash), Loss: 0.6937\n",
      "tensor([0.9967, 0.9929, 0.9863,  ..., 0.9930, 0.9549, 0.9945])\n",
      "tensor([0., 0., 0.,  ..., 1., 1., 1.])\n",
      "Epoch 16 (Wash), Loss: 0.6936\n",
      "tensor([0.9967, 0.9929, 0.9863,  ..., 0.9930, 0.9549, 0.9945])\n",
      "tensor([0., 0., 0.,  ..., 1., 1., 1.])\n",
      "Epoch 17 (Wash), Loss: 0.6936\n",
      "tensor([0.9967, 0.9929, 0.9863,  ..., 0.9930, 0.9549, 0.9945])\n",
      "tensor([0., 0., 0.,  ..., 1., 1., 1.])\n",
      "Epoch 18 (Wash), Loss: 0.6935\n",
      "tensor([0.9967, 0.9929, 0.9863,  ..., 0.9930, 0.9549, 0.9945])\n",
      "tensor([0., 0., 0.,  ..., 1., 1., 1.])\n",
      "Epoch 19 (Wash), Loss: 0.6935\n",
      "tensor([0.9967, 0.9929, 0.9863,  ..., 0.9930, 0.9549, 0.9945])\n",
      "tensor([0., 0., 0.,  ..., 1., 1., 1.])\n",
      "Epoch 20 (Wash), Loss: 0.6934\n",
      "Detected 10000 anomalies out of 200000 samples.\n",
      "Detected 5000 anomalies out of 100000 samples.\n",
      "tensor([0.9944, 0.9934, 0.9920,  ..., 0.9860, 0.9893, 0.9986])\n",
      "Epoch 1 (Lou), Loss: 0.7617\n",
      "tensor([0.9944, 0.9934, 0.9920,  ..., 0.9860, 0.9893, 0.9986])\n",
      "Epoch 2 (Lou), Loss: 0.7585\n",
      "tensor([0.9944, 0.9934, 0.9920,  ..., 0.9860, 0.9893, 0.9986])\n",
      "Epoch 3 (Lou), Loss: 0.7553\n",
      "tensor([0.9944, 0.9934, 0.9920,  ..., 0.9860, 0.9893, 0.9986])\n",
      "Epoch 4 (Lou), Loss: 0.7522\n",
      "tensor([0.9944, 0.9934, 0.9920,  ..., 0.9860, 0.9893, 0.9986])\n",
      "Epoch 5 (Lou), Loss: 0.7491\n",
      "tensor([0.9944, 0.9934, 0.9920,  ..., 0.9860, 0.9893, 0.9986])\n",
      "Epoch 6 (Lou), Loss: 0.7461\n",
      "tensor([0.9944, 0.9934, 0.9920,  ..., 0.9860, 0.9893, 0.9986])\n",
      "Epoch 7 (Lou), Loss: 0.7431\n",
      "tensor([0.9944, 0.9934, 0.9920,  ..., 0.9860, 0.9893, 0.9986])\n",
      "Epoch 8 (Lou), Loss: 0.7402\n",
      "tensor([0.9944, 0.9934, 0.9920,  ..., 0.9860, 0.9893, 0.9986])\n",
      "Epoch 9 (Lou), Loss: 0.7373\n",
      "tensor([0.9944, 0.9934, 0.9920,  ..., 0.9860, 0.9893, 0.9986])\n",
      "Epoch 10 (Lou), Loss: 0.7345\n",
      "tensor([0.9944, 0.9934, 0.9920,  ..., 0.9860, 0.9893, 0.9986])\n",
      "Epoch 11 (Lou), Loss: 0.7318\n",
      "tensor([0.9944, 0.9934, 0.9920,  ..., 0.9860, 0.9893, 0.9986])\n",
      "Epoch 12 (Lou), Loss: 0.7290\n",
      "tensor([0.9944, 0.9934, 0.9920,  ..., 0.9860, 0.9893, 0.9986])\n",
      "Epoch 13 (Lou), Loss: 0.7264\n",
      "tensor([0.9944, 0.9934, 0.9920,  ..., 0.9860, 0.9893, 0.9986])\n",
      "Epoch 14 (Lou), Loss: 0.7238\n",
      "tensor([0.9944, 0.9934, 0.9920,  ..., 0.9860, 0.9893, 0.9986])\n",
      "Epoch 15 (Lou), Loss: 0.7212\n",
      "tensor([0.9944, 0.9934, 0.9920,  ..., 0.9860, 0.9893, 0.9986])\n",
      "Epoch 16 (Lou), Loss: 0.7187\n",
      "tensor([0.9944, 0.9934, 0.9920,  ..., 0.9860, 0.9893, 0.9986])\n",
      "Epoch 17 (Lou), Loss: 0.7163\n",
      "tensor([0.9944, 0.9934, 0.9920,  ..., 0.9860, 0.9893, 0.9986])\n",
      "Epoch 18 (Lou), Loss: 0.7139\n",
      "tensor([0.9944, 0.9934, 0.9920,  ..., 0.9860, 0.9893, 0.9986])\n",
      "Epoch 19 (Lou), Loss: 0.7116\n",
      "tensor([0.9944, 0.9934, 0.9920,  ..., 0.9860, 0.9893, 0.9986])\n",
      "Epoch 20 (Lou), Loss: 0.7093\n",
      "Detected 5000 anomalies out of 100000 samples.\n",
      "Detected 5000 anomalies out of 100000 samples.\n",
      "torch.Size([10000, 1])\n",
      "torch.Size([10000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hapah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\hapah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\hapah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (Wash): 1.0000\n",
      "Precision (Wash): 0.0000\n",
      "Recall (Wash): 0.0000\n",
      "F1 Score (Wash): 0.0000\n",
      "Detected 10000 anomalies out of 200000 samples.\n",
      "Detected 5000 anomalies out of 100000 samples.\n",
      "Test Accuracy (Lou): 0.0000\n",
      "Precision (Lou): 0.0000\n",
      "Recall (Lou): 0.0000\n",
      "F1 Score (Lou): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hapah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "train_wash, test_wash = random_split(background_wash, [80000, 20000])\n",
    "train_bbh_wash, test_bbh_wash = random_split(bbh_wash, [80000, 20000])\n",
    "train_lou, test_lou = random_split(background_lou, [80000, 20000])\n",
    "train_bbh_lou, test_bbh_lou = random_split(bbh_lou, [80000, 20000])\n",
    "batch_size = 64\n",
    "\n",
    "# DataLoader for Wash\n",
    "train_loader_wash = DataLoader(train_wash, batch_size=batch_size, shuffle=True)\n",
    "test_loader_wash = DataLoader(test_wash, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_loader_bbh_wash = DataLoader(train_bbh_wash, batch_size=batch_size, shuffle=True)\n",
    "test_loader_bbh_wash = DataLoader(test_bbh_wash, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# DataLoader for Lou\n",
    "train_loader_lou = DataLoader(train_lou, batch_size=batch_size, shuffle=True)\n",
    "test_loader_lou = DataLoader(test_lou, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_loader_bbh_lou = DataLoader(train_bbh_lou, batch_size=batch_size, shuffle=True)\n",
    "test_loader_bbh_lou = DataLoader(test_bbh_lou, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 2. Training the Autoencoder (Separate Autoencoders for Wash and Lou)\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=20):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            inputs = batch[0]\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Autoencoder for Wash and Lou\n",
    "input_dim = background_wash.shape[1]\n",
    "autoencoder_wash = Autoencoder(input_dim)\n",
    "autoencoder_lou = Autoencoder(input_dim)\n",
    "criterion_ae = nn.MSELoss()\n",
    "optimizer_ae_wash = optim.Adam(autoencoder_wash.parameters(), lr=0.001)\n",
    "optimizer_ae_lou = optim.Adam(autoencoder_lou.parameters(), lr=0.001)\n",
    "\n",
    "# Train Autoencoder for Wash\n",
    "print(\"\\nTraining Autoencoder for Wash:\")\n",
    "train_model(autoencoder_wash, train_loader_wash, criterion_ae, optimizer_ae_wash, epochs=20)\n",
    "\n",
    "# Train Autoencoder for Lou\n",
    "print(\"\\nTraining Autoencoder for Lou:\")\n",
    "train_model(autoencoder_lou, train_loader_lou, criterion_ae, optimizer_ae_lou, epochs=20)\n",
    "\n",
    "def evaluate_model(model, test_loader, percentile_threshold=95):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        # If test_loader is a tensor, convert it to a DataLoader\n",
    "        if isinstance(test_loader, torch.Tensor):\n",
    "            test_loader = DataLoader(test_loader, batch_size=64, shuffle=False)\n",
    "        \n",
    "        for batch in test_loader:\n",
    "            inputs = batch if isinstance(batch, torch.Tensor) else batch[0]\n",
    "            if isinstance(model, Autoencoder):\n",
    "                reconstructed = model(inputs)\n",
    "                error = torch.mean((reconstructed - inputs) ** 2, dim=1)\n",
    "                predictions.extend(error.numpy())\n",
    "            else:\n",
    "                energy = model(inputs).squeeze()\n",
    "                predictions.extend(energy.numpy())\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    threshold = np.percentile(predictions, percentile_threshold)\n",
    "    anomalies = predictions > threshold\n",
    "    print(f\"Detected {np.sum(anomalies)} anomalies out of {len(predictions)} samples.\")\n",
    "    return predictions, threshold\n",
    "\n",
    "# 3. Prepare Anomaly Detection Data (Separate for Wash and Lou)\n",
    "def prepare_anomaly_data(autoencoder, normal_data, positive_data):\n",
    "    autoencoder.eval()\n",
    "    # Compute reconstruction errors\n",
    "    normal_errors = evaluate_model(autoencoder, normal_data)\n",
    "    positive_errors = evaluate_model(autoencoder, positive_data)\n",
    "    \n",
    "    # Combine errors and labels\n",
    "    X = np.concatenate([normal_errors[0], positive_errors[0]])\n",
    "    y = np.concatenate([np.zeros(len(normal_errors[0])), np.ones(len(positive_errors[0]))])\n",
    "    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "    # # Shuffle the data\n",
    "    # indices = np.arange(len(y))\n",
    "    # np.random.shuffle(indices)\n",
    "    # X, y = X[indices], y[indices]\n",
    "    \n",
    "    # return torch.tensor(X,  dtype=torch.float32), torch.tensor(y,  dtype=torch.float32)\n",
    "\n",
    "# 4. Train Classifier for Wash\n",
    "# normal_data_wash = torch.cat([background_wash, bbh_wash])  # Combining background and BBH for normal data\n",
    "normal_data_wash = background_wash\n",
    "positive_data_wash = sglf_wash  # Sine-Gaussian for positive anomalies\n",
    "\n",
    "X_train_wash, y_train_wash = prepare_anomaly_data(autoencoder_wash, normal_data_wash, positive_data_wash)\n",
    "\n",
    "\n",
    "# Classifier for Wash\n",
    "classifier_wash = nn.Sequential(\n",
    "    nn.Linear(1, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "classifier_wash = classifier_wash\n",
    "# Loss and optimizer for classifier (Wash)\n",
    "criterion_classifier_wash = nn.BCELoss()\n",
    "optimizer_classifier_wash = optim.Adam(classifier_wash.parameters(), lr=0.001)\n",
    "\n",
    "# Train classifier for Wash\n",
    "for epoch in range(20):\n",
    "    classifier_wash.train()\n",
    "    optimizer_classifier_wash.zero_grad()\n",
    "    outputs = classifier_wash(X_train_wash.view(-1, 1))\n",
    "    print(X_train_wash)\n",
    "    print(y_train_wash)\n",
    "    loss = criterion_classifier_wash(outputs.view(-1), y_train_wash.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer_classifier_wash.step()\n",
    "    print(f\"Epoch {epoch+1} (Wash), Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 5. Train Classifier for Lou\n",
    "normal_data_lou = torch.cat([background_lou, bbh_lou])  # Combining background and BBH for normal data\n",
    "positive_data_lou = sglf_lou  # Sine-Gaussian for positive anomalies\n",
    "\n",
    "X_train_lou, y_train_lou = prepare_anomaly_data(autoencoder_lou, normal_data_lou, positive_data_lou)\n",
    "\n",
    "# Classifier for Lou\n",
    "classifier_lou = nn.Sequential(\n",
    "    nn.Linear(1, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# Loss and optimizer for classifier (Lou)\n",
    "criterion_classifier_lou = nn.BCELoss()\n",
    "optimizer_classifier_lou = optim.Adam(classifier_lou.parameters(), lr=0.001)\n",
    "\n",
    "# Train classifier for Lou\n",
    "for epoch in range(20):\n",
    "    classifier_lou.train()\n",
    "    optimizer_classifier_lou.zero_grad()\n",
    "    outputs = classifier_lou(X_train_lou.view(-1, 1))\n",
    "    print(X_train_lou)\n",
    "    loss = criterion_classifier_lou(outputs.view(-1), y_train_lou.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer_classifier_lou.step()\n",
    "    print(f\"Epoch {epoch+1} (Lou), Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 6. Evaluate the Classifiers on Test Data (Separate for Wash and Lou)\n",
    "# Evaluate classifier for Wash\n",
    "# normal_data_test_wash = torch.cat([background_wash, bbh_wash])  # Combining background and BBH for normal data\n",
    "normal_data_test_wash = background_wash  # Combining background and BBH for normal data\n",
    "positive_data_test_wash = sglf_wash  # Sine-Gaussian for positive anomalies\n",
    "\n",
    "X_test_wash, y_test_wash = prepare_anomaly_data(autoencoder_wash, normal_data_test_wash, positive_data_test_wash)\n",
    "\n",
    "\n",
    "classifier_wash.eval()\n",
    "with torch.no_grad():\n",
    "    predictions_wash = classifier_wash(X_test_wash.view(-1, 1))\n",
    "    predicted_labels_wash = (predictions_wash > 0.5).float()\n",
    "    y_test_wash = y_test_wash\n",
    "    predicted_labels_wash = predicted_labels_wash[:10000]\n",
    "    y_test_wash = y_test_wash[:10000]\n",
    "    print(predicted_labels_wash.shape)\n",
    "    print(y_test_wash.shape)\n",
    "    accuracy_wash = (predicted_labels_wash == y_test_wash).float().mean()\n",
    "    precision_wash = precision_score(y_test_wash, predicted_labels_wash)\n",
    "    recall_wash = recall_score(y_test_wash, predicted_labels_wash)\n",
    "    f1_wash = f1_score(y_test_wash, predicted_labels_wash)\n",
    "    \n",
    "    print(f\"Test Accuracy (Wash): {accuracy_wash.item():.4f}\")\n",
    "    print(f\"Precision (Wash): {precision_wash:.4f}\")\n",
    "    print(f\"Recall (Wash): {recall_wash:.4f}\")\n",
    "    print(f\"F1 Score (Wash): {f1_wash:.4f}\")\n",
    "\n",
    "# Evaluate classifier for Lou\n",
    "normal_data_test_lou = torch.cat([background_lou, bbh_lou])  # Combining background and BBH for normal data\n",
    "positive_data_test_lou = sglf_lou  # Sine-Gaussian for positive anomalies\n",
    "\n",
    "X_test_lou, y_test_lou = prepare_anomaly_data(autoencoder_lou, normal_data_test_lou, positive_data_test_lou)\n",
    "\n",
    "classifier_lou.eval()\n",
    "with torch.no_grad():\n",
    "    predictions_lou = classifier_lou(X_test_lou.view(-1, 1))\n",
    "    predicted_labels_lou = (predictions_lou > 0.5).float()\n",
    "    y_test_lou = y_test_lou\n",
    "    predicted_labels_lou = predicted_labels_lou[:10000]\n",
    "    y_test_lou = y_test_lou[:10000]\n",
    "    accuracy_lou = (predicted_labels_lou == y_test_lou).float().mean()\n",
    "    precision_lou = precision_score(y_test_lou, predicted_labels_lou)\n",
    "    recall_lou = recall_score(y_test_lou, predicted_labels_lou)\n",
    "    f1_lou = f1_score(y_test_lou, predicted_labels_lou)\n",
    "    \n",
    "    print(f\"Test Accuracy (Lou): {accuracy_lou.item():.4f}\")\n",
    "    print(f\"Precision (Lou): {precision_lou:.4f}\")\n",
    "    print(f\"Recall (Lou): {recall_lou:.4f}\")\n",
    "    print(f\"F1 Score (Lou): {f1_lou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Autoencoder for Wash:\n",
      "Epoch 1/20, Loss: 1.1898\n",
      "Epoch 2/20, Loss: 1.0669\n",
      "Epoch 3/20, Loss: 1.0174\n",
      "Epoch 4/20, Loss: 1.0009\n",
      "Epoch 5/20, Loss: 0.9970\n",
      "Epoch 6/20, Loss: 0.9947\n",
      "Epoch 7/20, Loss: 0.9934\n",
      "Epoch 8/20, Loss: 0.9923\n",
      "Epoch 9/20, Loss: 0.9921\n",
      "Epoch 10/20, Loss: 0.9914\n",
      "Epoch 11/20, Loss: 0.9911\n",
      "Epoch 12/20, Loss: 0.9906\n",
      "Epoch 13/20, Loss: 0.9893\n",
      "Epoch 14/20, Loss: 0.9896\n",
      "Epoch 15/20, Loss: 0.9896\n",
      "Epoch 16/20, Loss: 0.9894\n",
      "Epoch 17/20, Loss: 0.9894\n",
      "Epoch 18/20, Loss: 0.9888\n",
      "Epoch 19/20, Loss: 0.9900\n",
      "Epoch 20/20, Loss: 0.9884\n",
      "\n",
      "Training Autoencoder for Lou:\n",
      "Epoch 1/20, Loss: 1.1895\n",
      "Epoch 2/20, Loss: 1.0724\n",
      "Epoch 3/20, Loss: 1.0196\n",
      "Epoch 4/20, Loss: 1.0024\n",
      "Epoch 5/20, Loss: 0.9972\n",
      "Epoch 6/20, Loss: 0.9942\n",
      "Epoch 7/20, Loss: 0.9935\n",
      "Epoch 8/20, Loss: 0.9925\n",
      "Epoch 9/20, Loss: 0.9911\n",
      "Epoch 10/20, Loss: 0.9912\n",
      "Epoch 11/20, Loss: 0.9901\n",
      "Epoch 12/20, Loss: 0.9908\n",
      "Epoch 13/20, Loss: 0.9909\n",
      "Epoch 14/20, Loss: 0.9891\n",
      "Epoch 15/20, Loss: 0.9891\n",
      "Epoch 16/20, Loss: 0.9900\n",
      "Epoch 17/20, Loss: 0.9905\n",
      "Epoch 18/20, Loss: 0.9890\n",
      "Epoch 19/20, Loss: 0.9887\n",
      "Epoch 20/20, Loss: 0.9884\n",
      "\n",
      "Training Energy-Based Model for Wash:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hapah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([200])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 1.0015\n",
      "Epoch 2/20, Loss: 1.0006\n",
      "Epoch 3/20, Loss: 1.0005\n",
      "Epoch 4/20, Loss: 1.0005\n",
      "Epoch 5/20, Loss: 1.0004\n",
      "Epoch 6/20, Loss: 1.0003\n",
      "Epoch 7/20, Loss: 1.0002\n",
      "Epoch 8/20, Loss: 1.0001\n",
      "Epoch 9/20, Loss: 1.0001\n",
      "Epoch 10/20, Loss: 1.0001\n",
      "Epoch 11/20, Loss: 1.0001\n",
      "Epoch 12/20, Loss: 1.0001\n",
      "Epoch 13/20, Loss: 1.0001\n",
      "Epoch 14/20, Loss: 1.0001\n",
      "Epoch 15/20, Loss: 1.0001\n",
      "Epoch 16/20, Loss: 1.0001\n",
      "Epoch 17/20, Loss: 1.0001\n",
      "Epoch 18/20, Loss: 1.0000\n",
      "Epoch 19/20, Loss: 1.0000\n",
      "Epoch 20/20, Loss: 1.0000\n",
      "\n",
      "Training Energy-Based Model for Lou:\n",
      "Epoch 1/20, Loss: 1.0016\n",
      "Epoch 2/20, Loss: 1.0005\n",
      "Epoch 3/20, Loss: 1.0003\n",
      "Epoch 4/20, Loss: 1.0002\n",
      "Epoch 5/20, Loss: 1.0002\n",
      "Epoch 6/20, Loss: 1.0002\n",
      "Epoch 7/20, Loss: 1.0001\n",
      "Epoch 8/20, Loss: 1.0001\n",
      "Epoch 9/20, Loss: 1.0001\n",
      "Epoch 10/20, Loss: 1.0001\n",
      "Epoch 11/20, Loss: 1.0001\n",
      "Epoch 12/20, Loss: 1.0001\n",
      "Epoch 13/20, Loss: 1.0001\n",
      "Epoch 14/20, Loss: 1.0001\n",
      "Epoch 15/20, Loss: 1.0001\n",
      "Epoch 16/20, Loss: 1.0001\n",
      "Epoch 17/20, Loss: 1.0001\n",
      "Epoch 18/20, Loss: 1.0001\n",
      "Epoch 19/20, Loss: 1.0001\n",
      "Epoch 20/20, Loss: 1.0001\n",
      "\n",
      "Evaluating Background Wash with Autoencoder:\n",
      "Detected 6 anomalies out of 200000 samples.\n",
      "Autoencoder - Threshold: 1.0272, Anomalies: 6\n",
      "Evaluating Background Wash with Energy-Based Model:\n",
      "Detected 36 anomalies out of 200000 samples.\n",
      "Energy Model - Threshold: 0.0593, Anomalies: 36\n",
      "\n",
      "Evaluating Background Lou with Autoencoder:\n",
      "Detected 6 anomalies out of 200000 samples.\n",
      "Autoencoder - Threshold: 1.0273, Anomalies: 6\n",
      "Evaluating Background Lou with Energy-Based Model:\n",
      "Detected 198 anomalies out of 200000 samples.\n",
      "Energy Model - Threshold: 0.0689, Anomalies: 198\n",
      "\n",
      "Evaluating BBH Wash with Autoencoder:\n",
      "Detected 28 anomalies out of 200000 samples.\n",
      "Autoencoder - Threshold: 1.0272, Anomalies: 28\n",
      "Evaluating BBH Wash with Energy-Based Model:\n",
      "Detected 1540 anomalies out of 200000 samples.\n",
      "Energy Model - Threshold: 0.0593, Anomalies: 1540\n",
      "\n",
      "Evaluating BBH Lou with Autoencoder:\n",
      "Detected 80 anomalies out of 200000 samples.\n",
      "Autoencoder - Threshold: 1.0273, Anomalies: 80\n",
      "Evaluating BBH Lou with Energy-Based Model:\n",
      "Detected 9808 anomalies out of 200000 samples.\n",
      "Energy Model - Threshold: 0.0689, Anomalies: 9808\n",
      "\n",
      "Evaluating SGLF Wash with Autoencoder:\n",
      "Detected 6 anomalies out of 200000 samples.\n",
      "Autoencoder - Threshold: 1.0272, Anomalies: 6\n",
      "Evaluating SGLF Wash with Energy-Based Model:\n",
      "Detected 40 anomalies out of 200000 samples.\n",
      "Energy Model - Threshold: 0.0593, Anomalies: 40\n",
      "\n",
      "Evaluating SGLF Lou with Autoencoder:\n",
      "Detected 6 anomalies out of 200000 samples.\n",
      "Autoencoder - Threshold: 1.0273, Anomalies: 6\n",
      "Evaluating SGLF Lou with Energy-Based Model:\n",
      "Detected 154 anomalies out of 200000 samples.\n",
      "Energy Model - Threshold: 0.0689, Anomalies: 154\n"
     ]
    }
   ],
   "source": [
    "# Data preparation\n",
    "train_wash, test_wash = random_split(background_wash, [80000, 20000])\n",
    "train_lou, test_lou = random_split(background_wash, [80000, 20000])\n",
    "\n",
    "batch_size = 128\n",
    "train_loader_wash = DataLoader(train_wash, batch_size=batch_size, shuffle=True)\n",
    "test_loader_wash = DataLoader(test_wash, batch_size=batch_size, shuffle=False)\n",
    "train_loader_lou = DataLoader(train_lou, batch_size=batch_size, shuffle=True)\n",
    "test_loader_lou = DataLoader(test_lou, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=20):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            inputs = batch[0]\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Model initialization and training\n",
    "input_dim = background_wash.shape[1]\n",
    "\n",
    "autoencoder_wash = Autoencoder(input_dim)\n",
    "criterion_ae = nn.MSELoss()\n",
    "optimizer_ae_wash = optim.Adam(autoencoder_wash.parameters(), lr=0.001)\n",
    "\n",
    "print(\"\\nTraining Autoencoder for Wash:\")\n",
    "train_model(autoencoder_wash, train_loader_wash, criterion_ae, optimizer_ae_wash, epochs=20)\n",
    "\n",
    "autoencoder_lou = Autoencoder(input_dim)\n",
    "optimizer_ae_lou = optim.Adam(autoencoder_lou.parameters(), lr=0.001)\n",
    "\n",
    "print(\"\\nTraining Autoencoder for Lou:\")\n",
    "train_model(autoencoder_lou, train_loader_lou, criterion_ae, optimizer_ae_lou, epochs=20)\n",
    "\n",
    "energy_model_wash = EnergyModel(input_dim)\n",
    "criterion_energy = nn.MSELoss()\n",
    "optimizer_energy_wash = optim.Adam(energy_model_wash.parameters(), lr=0.001)\n",
    "\n",
    "print(\"\\nTraining Energy-Based Model for Wash:\")\n",
    "train_model(energy_model_wash, train_loader_wash, criterion_energy, optimizer_energy_wash, epochs=20)\n",
    "\n",
    "energy_model_lou = EnergyModel(input_dim)\n",
    "optimizer_energy_lou = optim.Adam(energy_model_lou.parameters(), lr=0.001)\n",
    "\n",
    "print(\"\\nTraining Energy-Based Model for Lou:\")\n",
    "train_model(energy_model_lou, train_loader_lou, criterion_energy, optimizer_energy_lou, epochs=20)\n",
    "\n",
    "# Anomaly detection using statistical approach\n",
    "def evaluate_model_statistical(model, test_loader, baseline_data=None, sigma_multiplier=3):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    if baseline_data is not None:\n",
    "        baseline_loader = DataLoader(baseline_data, batch_size=128, shuffle=False) if isinstance(baseline_data, torch.Tensor) else baseline_data\n",
    "        baseline_scores = []\n",
    "        with torch.no_grad():\n",
    "            for batch in baseline_loader:\n",
    "                inputs = batch if isinstance(batch, torch.Tensor) else batch[0]\n",
    "                if isinstance(model, Autoencoder):\n",
    "                    reconstructed = model(inputs)\n",
    "                    error = torch.mean((reconstructed - inputs) ** 2, dim=1)\n",
    "                    baseline_scores.extend(error.numpy())\n",
    "                else:\n",
    "                    energy = model(inputs).squeeze()\n",
    "                    baseline_scores.extend(energy.numpy())\n",
    "        \n",
    "        baseline_scores = np.array(baseline_scores)\n",
    "        mean_baseline = np.mean(baseline_scores)\n",
    "        std_baseline = np.std(baseline_scores)\n",
    "        threshold = mean_baseline + sigma_multiplier * std_baseline\n",
    "    else:\n",
    "        threshold = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if isinstance(test_loader, torch.Tensor):\n",
    "            test_loader = DataLoader(test_loader, batch_size=128, shuffle=False)\n",
    "        for batch in test_loader:\n",
    "            inputs = batch if isinstance(batch, torch.Tensor) else batch[0]\n",
    "            if isinstance(model, Autoencoder):\n",
    "                reconstructed = model(inputs)\n",
    "                error = torch.mean((reconstructed - inputs) ** 2, dim=1)\n",
    "                predictions.extend(error.numpy())\n",
    "            else:\n",
    "                energy = model(inputs).squeeze()\n",
    "                predictions.extend(energy.numpy())\n",
    "                \n",
    "    with torch.no_grad():\n",
    "        # If test_loader is a tensor, convert it to a DataLoader\n",
    "        if isinstance(test_loader, torch.Tensor):\n",
    "            test_loader = DataLoader(test_loader, batch_size=128, shuffle=False)\n",
    "        \n",
    "        for batch in test_loader:\n",
    "            inputs = batch if isinstance(batch, torch.Tensor) else batch[0]\n",
    "            if isinstance(model, Autoencoder):\n",
    "                reconstructed = model(inputs)\n",
    "                error = torch.mean((reconstructed - inputs) ** 2, dim=1)\n",
    "                predictions.extend(error.numpy())\n",
    "            else:\n",
    "                energy = model(inputs).squeeze()\n",
    "                predictions.extend(energy.numpy())\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    if threshold is not None:\n",
    "        anomalies = predictions > threshold\n",
    "        print(f\"Detected {np.sum(anomalies)} anomalies out of {len(predictions)} samples.\")\n",
    "    else:\n",
    "        anomalies = np.zeros_like(predictions, dtype=bool)\n",
    "\n",
    "    return predictions, threshold, anomalies\n",
    "\n",
    "datasets = [\n",
    "    (\"Background Wash\", background_wash, autoencoder_wash, energy_model_wash),\n",
    "    (\"Background Lou\", background_lou, autoencoder_lou, energy_model_lou),\n",
    "    (\"BBH Wash\", bbh_wash, autoencoder_wash, energy_model_wash),\n",
    "    (\"BBH Lou\", bbh_lou, autoencoder_lou, energy_model_lou),\n",
    "    (\"SGLF Wash\", sglf_wash, autoencoder_wash, energy_model_wash),\n",
    "    (\"SGLF Lou\", sglf_lou, autoencoder_lou, energy_model_lou),\n",
    "]\n",
    "\n",
    "for name, data, autoencoder, energy_model in datasets:\n",
    "    print(f\"\\nEvaluating {name} with Autoencoder:\")\n",
    "    recon_errors, threshold_ae, anomalies_ae = evaluate_model_statistical(autoencoder, data, baseline_data=background_wash)\n",
    "    print(f\"Autoencoder - Threshold: {threshold_ae:.4f}, Anomalies: {np.sum(anomalies_ae)}\")\n",
    "\n",
    "    print(f\"Evaluating {name} with Energy-Based Model:\")\n",
    "    energy_scores, threshold_energy, anomalies_energy = evaluate_model_statistical(energy_model, data, baseline_data=background_wash)\n",
    "    print(f\"Energy Model - Threshold: {threshold_energy:.4f}, Anomalies: {np.sum(anomalies_energy)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
