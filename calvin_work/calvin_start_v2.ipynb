{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F \n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coded from scratch using https://www.youtube.com/watch?v=VELQT1-hILo&t=1395s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input wave --> hideen dim --> mean, std --> parametrization trick -> decoder -> output wave\n",
    "class VariationalAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, h_dim=200, z_dim=20):\n",
    "        super().__init__()\n",
    "        #encoder\n",
    "        self.wave_2hid = nn.Linear(input_dim, h_dim)\n",
    "        self.hid_2mu = nn.Linear(h_dim, z_dim)\n",
    "        self.hid_2sigma = nn.Linear(h_dim, z_dim)\n",
    "\n",
    "        #decoder\n",
    "        self.z_2hid = nn.Linear(z_dim, h_dim)\n",
    "        self.hid_2wave = nn.Linear(h_dim, input_dim)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.relu(self.wave_2hid(x))\n",
    "        mu, sigma = self.hid_2mu(h), self.hid_2sigma(h)\n",
    "        return  mu, sigma\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.relu(self.z_2hid(z))\n",
    "        # no need to sigmoid here i think because we are not trying to limit the range from 0 to 1\n",
    "        return self.hid_2wave(h)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, sigma = self.encode(x)\n",
    "        epsilon = torch.randn_like(sigma)\n",
    "        z_new = mu + sigma * epsilon\n",
    "        x_reconstructed = self.decode(z_new)\n",
    "        return x_reconstructed, mu, epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 400])\n",
      "torch.Size([4, 20])\n",
      "torch.Size([4, 20])\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "x = torch.randn(4, 200, 2)\n",
    "# 4 represents the number of data segments, 200 is the number of data points in each segment, 2 represents 2 different channels\n",
    "x_flat = x.view(x.size(0), -1) # reshape into (4, 200)\n",
    "vae = VariationalAutoEncoder(input_dim=400)\n",
    "x_reconstructed, mu, sigma = vae(x_flat)\n",
    "print(x_reconstructed.shape)\n",
    "print(mu.shape)\n",
    "print(sigma.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_dim = 400\n",
    "h_dim = 200\n",
    "z_dim = 20 # how much compression\n",
    "num_epochs = 10\n",
    "batch_size = 32 # when we train the NN, we only put in this number of samples into the network each time, we could increase the batch size \n",
    "lr_rate = 3e-5   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset loading\n",
    "background = np.load('../data/background.npz')['data']\n",
    "bbh = np.load('../data/bbh_for_challenge.npy')\n",
    "sglf = np.load('../data/sglf_for_challenge.npy')\n",
    "\n",
    "def normalize(data):\n",
    "    stds = np.std(data, axis=-1, keepdims=True)\n",
    "    return data / stds\n",
    "\n",
    "background = normalize(background)\n",
    "x_flat = x.view(x.size(0), -1)\n",
    "bbh = normalize(bbh)\n",
    "sglf = normalize(sglf)\n",
    "\n",
    "x_train, x_test = train_test_split(background, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
    "x_test = torch.tensor(x_test, dtype=torch.float32)\n",
    "bbh = torch.tensor(bbh, dtype=torch.float32)\n",
    "sglf = torch.tensor(sglf, dtype=torch.float32)\n",
    "\n",
    "x_train = x_train.view(x_train.size(0), -1)\n",
    "x_test = x_test.view(x_test.size(0), -1)\n",
    "bbh = bbh.view(bbh.size(0), -1)\n",
    "sglf = sglf.view(sglf.size(0), -1)\n",
    "\n",
    "train_loader = DataLoader(dataset=x_train, batch_size=batch_size)\n",
    "\n",
    "model = VariationalAutoEncoder(input_dim, h_dim, z_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr_rate, weight_decay=1e-5)\n",
    "loss_fn = nn.MSELoss(reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2500it [00:06, 368.39it/s, loss=1.32e+4]\n",
      "2500it [00:07, 356.34it/s, loss=1.29e+4]\n",
      "2500it [00:06, 363.50it/s, loss=1.27e+4]\n",
      "2500it [00:06, 385.08it/s, loss=1.26e+4]\n",
      "2500it [00:06, 380.73it/s, loss=1.25e+4]\n",
      "2500it [00:06, 381.71it/s, loss=1.24e+4]\n",
      "2500it [00:06, 398.50it/s, loss=1.25e+4]\n",
      "2500it [00:06, 376.85it/s, loss=1.24e+4]\n",
      "2500it [00:06, 377.87it/s, loss=1.23e+4]\n",
      "2500it [00:06, 376.63it/s, loss=1.24e+4]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    loop = tqdm(enumerate(train_loader))\n",
    "    for i, x in loop:\n",
    "        # forward pass\n",
    "        x = x.to(device).view(x.shape[0], input_dim) # keep the inital sample size of 100,000 but reshape into the input dim of 400\n",
    "        x_reconstructed, mu, sigma = model(x)\n",
    "\n",
    "        # compute loss\n",
    "        reconstruction_loss = loss_fn(x_reconstructed, x)\n",
    "        kl_div = -torch.sum(1 + torch.log(sigma.pow(2)) - mu.pow(2) - sigma.pow(2))\n",
    "\n",
    "        # backprop\n",
    "        loss = reconstruction_loss + kl_div # could add an alpha term to make it a disentangled autoencoder\n",
    "        optimizer.zero_grad() # ensure no gradients before\n",
    "        loss.backward() \n",
    "        optimizer.step() # compute new gradients\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reconstruction_errors(model, data):\n",
    "    model.eval()\n",
    "    data = data.to(device) \n",
    "    num_samples = data.size(0)\n",
    "    with torch.no_grad():\n",
    "        data = data.view(data.size(0), -1)\n",
    "        reconstructed, mu, sigma = model(data)\n",
    "        reconstruction_loss = loss_fn(reconstructed, data) / num_samples\n",
    "        kl_div = -torch.sum(1 + torch.log(sigma.pow(2)) - mu.pow(2) - sigma.pow(2)) / num_samples\n",
    "        total_loss = reconstruction_loss + kl_div\n",
    "    return reconstruction_loss.item(), kl_div.item(), total_loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test.to(device)\n",
    "sglf = sglf.to(device)\n",
    "bbh = bbh.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set - Reconstruction Loss: 360.8668518066406, KL Divergence: 27.032485961914062, Total Loss: 387.89935302734375\n",
      "SGLF Set - Reconstruction Loss: 355.3948059082031, KL Divergence: 27.199203491210938, Total Loss: 382.593994140625\n",
      "BBH Set - Reconstruction Loss: 370.8006591796875, KL Divergence: 26.72834014892578, Total Loss: 397.52899169921875\n"
     ]
    }
   ],
   "source": [
    "test_rec_loss, test_kl_loss, test_total_loss = compute_reconstruction_errors(model, x_test)\n",
    "sglf_rec_loss, sglf_kl_loss, sglf_total_loss = compute_reconstruction_errors(model, sglf)\n",
    "bbh_rec_loss, bbh_kl_loss, bbh_total_loss = compute_reconstruction_errors(model, bbh)\n",
    "\n",
    "print(f\"Test Set - Reconstruction Loss: {test_rec_loss}, KL Divergence: {test_kl_loss}, Total Loss: {test_total_loss}\")\n",
    "print(f\"SGLF Set - Reconstruction Loss: {sglf_rec_loss}, KL Divergence: {sglf_kl_loss}, Total Loss: {sglf_total_loss}\")\n",
    "print(f\"BBH Set - Reconstruction Loss: {bbh_rec_loss}, KL Divergence: {bbh_kl_loss}, Total Loss: {bbh_total_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally the loss should be significantly higher for the test set compared to the sglf and bbh sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smash",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
